---
layout: post
title: 服务器下复现UniGoal
date: 2025-04-25 14:32:13
description: 服务器下复现UniGoal
tags: Debug Deploy
categories: Experience
tabs: true
---

# UniGoal

在机器人导航领域，现有方法通常针对特定任务（如物体导航、图像导航或文本导航）设计独立框架，导致模型泛化能力受限且难以应对真实场景中复杂的多模态指令。为此，本研究提出UniGoal ——首个零样本学习的通用目标导航框架，通过统一的图表示与大语言模型（LLM）推理 ，实现了跨物体类别、图像和文本描述三类任务的零样本导航。其核心创新在于：（1）构建动态场景图与目标图的统一表示，将环境感知与目标描述转化为结构化图，保留丰富的空间与语义关系；（2）设计[多阶段探索策略](https://zhida.zhihu.com/search?content_id=255217302&content_type=Article&match_order=1&q=多阶段探索策略&zhida_source=entity)，根据图匹配程度动态调整探索策略，在零匹配阶段通过子图分解逐步探索未知区域，部分匹配时利用坐标投影与锚点对齐推断目标位置，完全匹配时通过场景图修正与验证确保定位准确。此外，引入[黑名单机制](https://zhida.zhihu.com/search?content_id=255217302&content_type=Article&match_order=1&q=黑名单机制&zhida_source=entity)避免重复探索失败区域，显著提升效率。

UniGoal的推理流程以图匹配为核心，结合LLM的推理能力实现高效决策。在场景图构建中，实时融合RGB-D观测中的语义信息，形成拓扑结构。目标图则根据输入类型（物体类别、图像或文本）通过LLM/VLM处理生成，确保与场景图表示的一致性。实验表明，UniGoal在[MatterPort3D](https://zhida.zhihu.com/search?content_id=255217302&content_type=Article&match_order=1&q=MatterPort3D&zhida_source=entity)、[HM3D](https://zhida.zhihu.com/search?content_id=255217302&content_type=Article&match_order=1&q=HM3D&zhida_source=entity)等数据集上全面超越现有零样本的专用任务方法（如[Mod-IIN](https://zhida.zhihu.com/search?content_id=255217302&content_type=Article&match_order=1&q=Mod-IIN&zhida_source=entity)[1]、SG-Nav[2]）及需训练的通用方法（如[GOAT](https://zhida.zhihu.com/search?content_id=255217302&content_type=Article&match_order=1&q=GOAT&zhida_source=entity)[3]），在物体导航（成功率41.0%）、图像实例导航（成功率60.2%）和文本导航（成功率20.2%）任务中均达最优性能。其无需训练的特性与多模态通用性，为机器人在未知环境中的灵活导航提供了全新范式，拥有在真实场景落地的潜力。

——[CVPR2025 | UniGoal：通用零样本目标导航，Navigate to Any Goal! - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/30973430092)



## 环境配置

### 一、Conda

```shell
git clone https://github.com/bagh2178/UniGoal.git
cd UniGoal
conda create -n unigoal python==3.8
conda activate unigoal

# AutoDL的学术加速
source /etc/network_turbo
unset http_proxy && unset https_proxy
```
### 二、Habitat-sim-0.2.3  &  Habitat-lab

请注意，这里的Python一定要是`==3.8`

```shell
conda install habitat-sim==0.2.3 -c conda-forge -c aihabitat
pip install -e third_party/habitat-lab
```

### 三、安装第三方包

#### 1. **LightGlue** (`github.com/cvg/LightGlue.git`)

功能：一种基于深度学习的局部特征匹配算法，专为稀疏特征匹配设计。它通过自适应计算机制动态调整推理深度和特征点数量，显著提升了匹配效率和精度

- 支持自省机制，可提前终止简单图像对的推理过程
- 在3D重建、SLAM等对实时性要求高的场景中表现优异

#### 2. **Detectron2** (`github.com/facebookresearch/detectron2.git`)

功能：Meta开源的计算机视觉工具库，支持目标检测、实例分割、关键点检测等任务

- 提供预训练模型如Faster R-CNN、Mask R-CNN等
- 支持灵活的配置系统和分布式训练

#### 3. **Grounded-Segment-Anything** (`IDEA-Research/Grounded-Segment-Anything`)

核心组件:

- **Segment Anything (SAM)**

  功能：Meta提出的通用图像分割模型，支持零样本分割（无需预训练即可分割任意物体）

  模型文件：`sam_vit_h_4b8939.pth`

  是其预训练权重，适用于高精度分割任务

- **GroundingDINO**

  功能：基于文本提示的零样本目标检测模型，可将自然语言描述与图像中的物体关联

  - **模型文件**：`groundingdino_swint_ogc.pth`是其预训练权重，支持如“检测戴帽子的人”等复杂语义查询

#### 4. **依赖项与模型文件**

`segment_anything`：SAM的PyTorch实现，提供图像分割基础功能

`GroundingDINO依赖`：包含文本-图像特征融合模块，支持多模态目标检测

模型文件用途：

- `sam_vit_h_4b8939.pth`：SAM的ViT-Huge模型权重，适合高精度分割。
- `groundingdino_swint_ogc.pth`：GroundingDINO的Swin Transformer权重，支持文本引导检测。

```shell
pip install git+https://github.com/cvg/LightGlue.git
pip install git+https://github.com/facebookresearch/detectron2.git # 请保证有足够的运行内存编译
git clone https://github.com/IDEA-Research/Grounded-Segment-Anything.git third_party/Grounded-Segment-Anything
cd third_party/Grounded-Segment-Anything
git checkout 5cb813f # 会得到一个警告：分离头指针状态，不用理会
pip install -e segment_anything
pip install --no-build-isolation -e GroundingDINO
cd ../../
mkdir -p data/models/  # -p 参数自动创建多级目录，这里原作者没搞，wget -O 参数指定的是完整文件路径，不会自动创建目录
wget -O data/models/sam_vit_h_4b8939.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
# data/models/sam_vit_h_4b 100%[=================================>]   2.39G  13.5MB/s    in 3m 9s

# source /etc/network_turbo 这里需要VPN ↓
wget -O data/models/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
# data/models/groundingdin 100%[=================================>] 661.85M   121MB/s    in 5.5s
```
### 四、安装别的小东西

`pytorch::faiss-gpu` 是 Meta AI 开发的 **GPU 加速版 Faiss 库**，专为大规模向量相似性搜索和聚类设计，可能和图搜索有关系

```shell
conda install pytorch::faiss-gpu
pip install -r requirements.txt
```
至此，环境配置完成。



## 数据集（HM3D）

### 数据集下载

为了数据集组织正确，先创建文件夹：

```
mkdir -p data/datasets/instance_imagenav/hm3d/v3/
mkdir -p data/scene_datasets/hm3d_v0.2/val/
```

然后，下载两个压缩包：

https://dl.fbaipublicfiles.com/habitat/data/datasets/imagenav/hm3d/v3/instance_imagenav_hm3d_v3.zip

https://mp-app-prod.s3.amazonaws.com/habitat/v1.0/hm3d-val-habitat-v0.2.tar

最后，对于`instance_imagenav_hm3d_v3.zip`，把`val`、`train`和`val_mini`复制到`data/datasets/instance_imagenav/hm3d/v3/`下面

对于`hm3d-val-habitat-v0.2.tar`，一样操作

注意，需要这么组织数据集：

```shell
UniGoal/
└── data/
    ├── datasets/
    │   └── instance_imagenav/
    │       └── hm3d/
    │           └── v3/
    │               └── val/
    │                   ├── content/
    │                   │   ├── 4ok3usBNeis.json.gz
    │                   │   ├── 5cdEh9F2hJL.json.gz
    │                   │   ├── ...
    │                   │   └── zt1RVoi7PcG.json.gz
    │                   └── val.json.gz
    └── scene_datasets/
        └── hm3d_v0.2/
            └── val/
                ├── 00800-TEEsavR23oF/
                │   ├── TEEsavR23oF.basis.glb
                │   └── TEEsavR23oF.basis.navmesh
                ├── 00801-HaxA7YrQdEC/
                ├── ...
                └── 00899-58NLZxWBSpk/
```


## **LLM and VLM**

Option 1: Install Ollama.【我们这里使用这个选项】

```sh
# source /etc/network_turbo
curl -fsSL https://ollama.com/install.sh | sh

# 如果你没有GPU的情况下，ollama会自行关闭，需要重新启动
# ollama serve &

ollama pull llama3.2-vision
```

![image-20250425170952781](S:\Programming\Son4ta.github.io\Son4ta.github.io\assets\img\image-20250425170952781.png)

Option 2: Use LLM and VLM via your own API. Change the `llm_model`, `vlm_model`, `api_key`, `base_url` in the configuration file `configs/config_habitat.yaml` to your own.



## Evaluation

```sh
CUDA_VISIBLE_DEVICES=0 python main.py  # instance-image-goal
```